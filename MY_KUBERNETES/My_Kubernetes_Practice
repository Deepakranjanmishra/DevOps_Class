              **MY_PRACTICE_COMMANDS_OUTPUTS_SCREENSHOTS**
Date:-10.5.2024
kubectl create deploy mydeploy --image=nginx:latest --port=80
kubectl get all
kubectl get deploy
kubectl get rs
kubectl get po
kubectl scale deploy mydeploy --replicas=2
kubectl --dryget po
kubectl scale depl

oy mydeploy --replicas=5
kubectl get po
kubectl scale deploy mydeploy --replicas=3
kubectl get po
kubectl scale deploy mydeploy --replicas=2
kubectl get po
kubectl scale deploy mydeploy --replicas=1
kubectl get po
kubectl scale deploy mydeploy --replicas=0
kubectl get po
k get all
kubectl get all
k get all
kubectl get all
kubectl delete deploy mydeploy
kubectl get all
clear
kubectl create deploy mydeploy --image=nginx:latest --port=80
kubectl get all
kubectl delete po mydeploy-5b477bf648-blx7t
kubectl get all
kubectl delete po mydeploy-5b477bf648-fst5p
kubectl get all
claer
clear
kubectl get all
kubectl create deploy mydeploy --image=nginx:latest --port=80 --dry-run=client -o yaml > mydeploy.yaml
ls
vi mydeploy.yaml 
kubectl scale deploy mydeploy --replicas=0
vi mydeploy.yaml 
kubectl scale deploy mydeploy --replicas=0 -o yaml > mydeploy2.yaml
ls
vi mydeploy2.yaml 
root@Master:/home/labsuser#
kubectl get all
  181  vi mydeploy.yaml 
  182  kubectl apply -f mydeploy.yaml 
  183  kubectl get all
  184  vi mydeploy.yaml 
  185  kubectl apply -f mydeploy.yaml 
  186  kubectl get all
  187  kubectl apply -f mydeploy.yaml 
  188  history

Date :- 11.5.2024
HPA
clear
kubectl create deploy mydeploy --image=nginx:latest --port=80
kubectl get all
kubectl delete po first second name-second
kubectl get all
# AUTO SCALING
kubectl autoscale deploy mydeploy --min=2 --max-10 --cpu-percent=80
kubectl autoscale deploy mydeploy --min=2 --max=10 --cpu-percent=10
kubectl get all
kubectl get hpa
kubectl autoscale deploy mydeploy --min=2 --max-10 --cpu-percent=86    ->error
kubectl autoscale deploy mydeploy --min=2 --max=10  --cpu-percent=86
kubectl get all
kubectl get hpa 
kubectl get hpa mydeploy --watch
kubectl edit hpa 
kubectl autoscale deploy mydeploy --min=2 --max-10 --cpu-percent=86
kubectl autoscale deploy mydeploy --min=2 --max=10 --cpu-percent=86
kubectl edit hpa 
kubectl edit hpa mydeploy
kubectl edit hpa mydeploy --cpu-percent=85
kubectl top po
kubectl get all
kubectl top pod
kubectl top pods
kubectl edit hpa mydeploy
kubectl get hpa
kubectl get all
history | cut -c 8-


kubectl get nodes
   68  kubectl get all
   69  kubectl delete hpa mydeploy

  ROLL BACK :
https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#pausing-and-resuming-a-deployment
Read the above documentation
   70  kubectl get all
   71  kubectl rollout status
   72  kubectl rollout status deploy mydeploy
   73  kubectl rollout history deploy mydeploy
   74  kubectl rollout status
   75  kubectl rollout history deploy mydeploy
   76  kubectl scale deploy mydeploy --replicas=20
   77  kubectl get all
   78  kubectl describe po mydeploy-5b477bf648-7jzvs
   79  kubectl set image deploy mydeploy nginx=nginx:1.8.1 --record
   80  kubectl rollout status deploy mydeploy
   81  kubectl get all
   82  kubectl describe po mydeploy-6b7b846487-569n7
   83  kubectl rollout history deploy mydeploy
   84  kubectl set image deploy mydeploy nginx=nginx:1.9.1 --record
   85  kubectl rollout status deploy mydeploy
   86  kubectl get all
   87  kubectl describe po mydeploy-
   88  kubectl describe po mydeploy-5f69579954-xsglx
   89  kubectl rollout history deploy mydeploy
   90  kubectl rollout status deploy mydeploy
   91  kubectl get all
   92  kubectl rollout history deploy mydeploy
   93  kubectl rollout undo deploy mydeploy
   94  kubectl rollout history deploy mydeploy
   95  kubectl rollout status deploy mydeploy
   96  kubectl get all
   97  kubectl describe po mydeploy-6b7b846487-twqv8
   98  kubectl rollout history deploy mydeploy
   99  kubectl rollout undo deploy mydeploy --revision=1
  100  kubectl rollout undo deploy mydeploy --to-revision=1
  101  kubectl rollout history deploy mydeploy
  102  kubectl rollout status deploy mydeploy
  103  kubectl get all
  104  kubectl rollout status deploy mydeploy
  105  k describe po mydeploy-5b477bf648-xwnpm
  106  kubectl describe po mydeploy-5b477bf648-xwnpm
  107  history
  108  kubectl rollout history deploy mydeploy
  109  kubectl rollout undo deploy mydeploy --to-revision=3
  110  kubectl rollout history deploy mydeploy
  111* kubectl ge
  112  k describe po mydeploy-5f69579954-xtg8d 
  113  kubectl describe po mydeploy-5f69579954-xtg8d 
  114  history
root@Master:/home/labsuser#

# DATE: 12.5.2024 Practice
  129  # SCHEDULING - NODE SELECTOR
  130  kubectl get all
  131  kubectl delete deploy mydeploy
  132  kubectl get all
  133  ls
  134    kubectl run first --image=nginx:latest --port=80 -o yaml > pod.yaml
  135  kubectl run first --image=nginx:latest --port=80 -o yaml > pod.yaml
  136  kubectl get po
  137  ls
  138  vi pod.yaml 
  139  kubectl delete po first
  140  kubectl get po
  141  kubectl get all
  142  ls
  143  vi pod.yaml 
  144  kubectl get nodes
  145  kubectl get nodes --showlabels
  146  kubectl get nodes --show-labels
  147  kubectl label no worker-node-1 hdd=ssd
  148  kubectl get nodes --show-labels
  149  kubectl apply -f pod.yaml
  150  kubectl get po
  151  kubectl get po -o wide
  152  # REMOVE THE LABEL
  153  kubectl describe po first
  154  kubectl label no worker-node-1 hdd- # THIS IS THE COMMAND TO REMOVE THE LABEL FRON THE PARTICULAR NODE
  155  kubectl get nodes --show-labels
  156  kubectl get no
  157  kubectl label no master hdd=ssd
  158  kubectl get nodes --show-labels
  159  kubectl get po
  160  kubectl delete po first
  161  kubectl get po
  162  kubectl apply -f pod.yaml
  163  kubectl get po
  164  kubectl describe po first
  165  kubectl get po
  166  kubectl get all
  167  kubectl label no master hdd-
  168  kubectl get po
  169  kubectl get nodes --show-labels
  170  kubectl get po
  171  kubectl get po -o wide
  172  kubectl get po -o wide
  173  kubectl get nodes --show-labels
  174  kubectl apply -f pod.yaml
  175  kubectl get po -o wide
  176  kubectl describe po first
  177  kubectl get po
  178  kubectl apply -f pod.yaml
  179  kubectl get po
  180  kubectl get nodes --show-labels
  181  kubectl get no
  182  kubectl label no worker-node-1 hdd=ssd
  183  kubectl get nodes --show-labels
  184  kubectl get po
  185  kubectl get po -o wide
  186  history
root@Master:/home/labsuser#
…………………………………………………………………………………………………………..
    # DATE :- 13.5.2024
      kubectl get all
  188  kubetl get po
  189  kubectl get po
  190  kubectl get no --show-labels
  191  kubectl get po -o wide
  192  cat pod.yaml 
  193  kubectl label no worker-node-1 hdd-
  194  kube ctl get po
  195  kubectl get po
  196  kubectl get po -o wide
  197  kubectl get no --show-labels
  198  kubectl get no worker-node-1 --show-labels
  200  kubectl get po -o wide
  201  kubectl apply -f pod.yaml 
  202  kubectl get po -o wide
  203  clear
  204  kubectl explain po
  205  kubectl explain po.spec
  206  kubectl explain po.spec.affinity
  207  kubectl explain po.spec.affinity.nodeAffinity
root@Master:/home/labsuser#     kubectl explain po.spec.affinity.nodeAffinity
KIND:       Pod
VERSION:    v1

FIELD: nodeAffinity <NodeAffinity>

DESCRIPTION:
    Describes node affinity scheduling rules for the pod.
    Node affinity is a group of node affinity scheduling rules.
    
FIELDS:
  preferredDuringSchedulingIgnoredDuringExecution       <[]PreferredSchedulingTerm>
    The scheduler will prefer to schedule pods to nodes that satisfy the
    affinity expressions specified by this field, but it may choose a node that
    violates one or more of the expressions. The node that is most preferred is
    the one with the greatest sum of weights, i.e. for each node that meets all
    of the scheduling requirements (resource request, requiredDuringScheduling
    affinity expressions, etc.), compute a sum by iterating through the elements
    of this field and adding "weight" to the sum if the node matches the
    corresponding matchExpressions; the node(s) with the highest sum are the
    most preferred.

  requiredDuringSchedulingIgnoredDuringExecution        <NodeSelector>
    If the affinity requirements specified by this field are not met at
    scheduling time, the pod will not be scheduled onto the node. If the
    affinity requirements specified by this field cease to be met at some point
    during pod execution (e.g. due to an update), the system may or may not try
    to eventually evict the pod from its node.
My written spec:
  uid: dc31a68e-e1ae-4cd7-8057-0feece765466
spec:
  affinity:
          nodeAffinity:
                  requiredDuringSchedulingIgnoredDuringExecution:
                                                 nodeSelectorTerms:
                                                      - matchExpressions:
                                                              - key: hdd
                                                                operator: In
                                                                values:
                                                                      - ssd
 

vi pod.yaml 
  233  kubectl apply -f pod.yaml 
  234  cat pod.yaml 
  235  kubectl label no worker-node-2 hdd=ssd
  236* 
  237  kubectl label no worker-node-2 hdd-
  238  kubectl get no --show-labels
  239  kubectl get po
  240  kubectl get po -o wide
  241  kubectl delete po first
  242  ls
  243  kubectl apply -f pod.yaml 
  244  kubectl get po -o wide
DATE:- 14/5/2024

See note…..
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
           matchExpressions:
             - key: run
               operator: In
               values:
                 - nginx
          topologyKey: kubernetes.io/hostname
w
root@Master:/home/labsuser# cat aff.yaml 
apiVersion: v1
kind: Pod
metadata:
  name: pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
           matchExpressions:
             - key: run
               operator: In
               values:
                 - nginx
          topologyKey: kubernetes.io/hostname       

  containers:
  - image: rajendrait99/first:1.0
    name: first
    ports:
    - containerPort: 8080
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
root@Master:/home/labsuser#




DATE : 16.5.2024
kubectl apply -f pod.yaml 
  118  kubectl get po -o wide
  119  kubectl delete po pod-1
  120  kubectl get po -o wide
  121  kubectl get no
  122  kubectl taint node node-1 key=value:NoSchedule
  123  kubectl describe no node-1
  124  kubectl apply -f pod2.yaml 
  125  kubectl get po -o wide
  126  kubectl get no
  127  ls
  128  kubectl cordon node node-2
  129  kubectl cordon node-2
  130  kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
  131  kubectl apply -f pod3.yaml 
  132  kubectl get po -o wide
  133  kubectl describe no node-2
  134  kubectl describe no node-1
  135  vi pod3.yaml 
  136  kubectl get po
  137  kubectl delete po pod3
  138  kubectl get po
  139  kubectl apply -f pod3.yaml 
  140  vi pod3.yaml 
  141  kubectl apply -f pod3.yaml 
  142  kubectl aget po -o wide
  143  kubectl get po -o wide
  144  kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
  145  vi pod3.yaml 
  146  vi pod4.yaml 
  147  kubectl uncordon node-2
  148  kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
  149  kubectl taint no node-2 key1=value1:NoSchedule
  150  kubectl describe node-2
  151  kubectl describeno  node-2
  152  kubectl describe no node-2
  153  vi pod3.yaml 
  154  kubectl get no -o wide
  155  kubectl get po -o wide
  156  kubectl delete po pod3.yaml 
  157  kubectl delete po pod3
  158  kubectl get po -o wide
  159  kubectl apply -f pod3.yaml 
  160  kubectl get po -o wide
  161  kubectl cordon node-1
  162  kubectl delete po pod3
  163  kubectl get po -o wide
  164  kubectl apply -f pod3.yaml 
  165  kubectl get po -o wide
  166  cat pod3.yaml 
  167  kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
  168  kubectl describe no node-1
root@Master:/home/labsuser#

root@Master:/home/labsuser# cat pod.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod-1
  name: pod-1
spec:
  containers:
  - image: nginx:latest
    name: pod-1
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
root@Master:/home/labsuser#

DATE: - 17.5.2024
PRACTICE:
labsuser@Master:~$ sudo -s
root@Master:/home/labsuser# kubectl get po -o wide
NAME   READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          20m     192.168.84.131   node-1   <none>           <none>
pod2   1/1     Running   0          17m     192.168.247.5    node-2   <none>           <none>
pod3   1/1     Running   0          8m42s   192.168.84.132   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
NAME     TAINT        KEY                                     VALUE
master   NoSchedule   node-role.kubernetes.io/control-plane   <none>
node-1   NoSchedule   key                                     value
node-2   NoSchedule   node.kubernetes.io/unschedulable        <none>
root@Master:/home/labsuser# vi pod3.yaml 
root@Master:/home/labsuser# vi pod4.yaml 
root@Master:/home/labsuser# vi pod4.yaml 
root@Master:/home/labsuser# kubectl uncordon node-2
node/node-2 uncordoned
root@Master:/home/labsuser# kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
NAME     TAINT        KEY                                     VALUE
master   NoSchedule   node-role.kubernetes.io/control-plane   <none>
node-1   NoSchedule   key                                     value
node-2   <none>       <none>                                  <none>
root@Master:/home/labsuser# kubectl taint no node-2 key1=value1:NoSchedule
node/node-2 tainted
root@Master:/home/labsuser# kubectl describe node-2
error: the server doesn't have a resource type "node-2"
root@Master:/home/labsuser# kubectl describeno  node-2
error: unknown command "describeno" for "kubectl"

Did you mean this?
        describe
root@Master:/home/labsuser# kubectl describe no node-2
Name:               node-2
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node-2
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.31.30.185/20
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.247.0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 16 May 2024 14:22:52 +0000
Taints:             key1=value1:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  node-2
  AcquireTime:     <unset>
  RenewTime:       Fri, 17 May 2024 07:03:12 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 17 May 2024 06:00:00 +0000   Fri, 17 May 2024 06:00:00 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 17 May 2024 07:01:16 +0000   Thu, 16 May 2024 14:22:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 17 May 2024 07:01:16 +0000   Thu, 16 May 2024 14:22:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 17 May 2024 07:01:16 +0000   Thu, 16 May 2024 14:22:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 17 May 2024 07:01:16 +0000   Fri, 17 May 2024 06:00:03 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.31.30.185
  Hostname:    node-2
Capacity:
  cpu:                2
  ephemeral-storage:  20134592Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8019624Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  18556039957
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7917224Ki
  pods:               110
System Info:
  Machine ID:                 ec23f4405d7a5a649897db3034c844a3
  System UUID:                ec21e678-cb3f-2e71-7ec6-70c3beba75a3
  Boot ID:                    aacc5929-775d-4067-8066-5ee64292c0e9
  Kernel Version:             6.2.0-1013-aws
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.8
  Kubelet Version:            v1.28.2
  Kube-Proxy Version:         v1.28.2
Non-terminated Pods:          (4 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     pod2                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 calico-kube-controllers-658d97c59c-s2t2d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h
  kube-system                 calico-node-c4qpv                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         16h
  kube-system                 kube-proxy-b48pv                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                250m (12%)  0 (0%)
  memory             0 (0%)      0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason              Age                  From     Message
  ----    ------              ----                 ----     -------
  Normal  NodeNotSchedulable  29m (x2 over 63m)    kubelet  Node node-2 status is now: NodeNotSchedulable
  Normal  NodeSchedulable     2m55s (x2 over 36m)  kubelet  Node node-2 status is now: NodeSchedulable
root@Master:/home/labsuser# vi pod3.yaml 
root@Master:/home/labsuser# kubectl get no -o wide
NAME     STATUS   ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
master   Ready    control-plane   16h   v1.28.2   172.31.31.141   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
node-1   Ready    <none>          16h   v1.28.2   172.31.22.151   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
node-2   Ready    <none>          16h   v1.28.2   172.31.30.185   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
root@Master:/home/labsuser# kubectl get po -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          36m   192.168.84.131   node-1   <none>           <none>
pod2   1/1     Running   0          33m   192.168.247.5    node-2   <none>           <none>
pod3   1/1     Running   0          24m   192.168.84.132   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl delete po pod3.yaml 
Error from server (NotFound): pods "pod3.yaml" not found
root@Master:/home/labsuser# kubectl delete po pod3
pod "pod3" deleted
root@Master:/home/labsuser# kubectl get po -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          37m   192.168.84.131   node-1   <none>           <none>
pod2   1/1     Running   0          34m   192.168.247.5    node-2   <none>           <none>
root@Master:/home/labsuser# kubectl apply -f pod3.yaml 
pod/pod3 created
root@Master:/home/labsuser# kubectl get po -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          37m   192.168.84.131   node-1   <none>           <none>
pod2   1/1     Running   0          34m   192.168.247.5    node-2   <none>           <none>
pod3   1/1     Running   0          4s    192.168.84.133   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl cordon node-1
node/node-1 cordoned
root@Master:/home/labsuser# kubectl delete po pod3
pod "pod3" deleted
root@Master:/home/labsuser# kubectl get po -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          38m   192.168.84.131   node-1   <none>           <none>
pod2   1/1     Running   0          35m   192.168.247.5    node-2   <none>           <none>
root@Master:/home/labsuser# kubectl apply -f pod3.yaml 
pod/pod3 created
root@Master:/home/labsuser# kubectl get po -o wide
NAME   READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
pod1   1/1     Running   0          38m   192.168.84.131   node-1   <none>           <none>
pod2   1/1     Running   0          35m   192.168.247.5    node-2   <none>           <none>
pod3   1/1     Running   0          5s    192.168.247.6    node-2   <none>           <none>
root@Master:/home/labsuser# cat pod3.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: pod3
  name: pod3
spec:
  tolerations:
   - key: key
     operator: Equal
     value: value
     effect: NoSchedule 

   - key: key1
     operator: Equal
     value: value1
     effect: NoSchedule  
  containers:
  - image: nginx:latest
    name: pod-1
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
root@Master:/home/labsuser# kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
NAME     TAINT        KEY                                     VALUE
master   NoSchedule   node-role.kubernetes.io/control-plane   <none>
node-1   NoSchedule   key                                     value
node-2   NoSchedule   key1                                    value1
root@Master:/home/labsuser# kubectl describe no node-1
Name:               node-1
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node-1
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.31.22.151/20
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.84.128
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 16 May 2024 14:22:47 +0000
Taints:             key=value:NoSchedule
                    node.kubernetes.io/unschedulable:NoSchedule
Unschedulable:      true
Lease:
  HolderIdentity:  node-1
  AcquireTime:     <unset>
  RenewTime:       Fri, 17 May 2024 07:12:12 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 17 May 2024 06:00:00 +0000   Fri, 17 May 2024 06:00:00 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 17 May 2024 07:11:22 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 17 May 2024 07:11:22 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 17 May 2024 07:11:22 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 17 May 2024 07:11:22 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.31.22.151
  Hostname:    node-1
Capacity:
  cpu:                2
  ephemeral-storage:  20134592Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8019636Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  18556039957
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7917236Ki
  pods:               110
System Info:
  Machine ID:                 ec23f4405d7a5a649897db3034c844a3
  System UUID:                ec2d9506-1341-104c-cdd8-caa7ad48451e
  Boot ID:                    1e7426a1-b7d8-4dcf-a470-98637aa07a5b
  Kernel Version:             6.2.0-1013-aws
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.8
  Kubelet Version:            v1.28.2
  Kube-Proxy Version:         v1.28.2
Non-terminated Pods:          (3 in total)
  Namespace                   Name                 CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                 ------------  ----------  ---------------  -------------  ---
  default                     pod1                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         43m
  kube-system                 calico-node-7dh66    250m (12%)    0 (0%)      0 (0%)           0 (0%)         16h
  kube-system                 kube-proxy-jndvz     0 (0%)        0 (0%)      0 (0%)           0 (0%)         16h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                250m (12%)  0 (0%)
  memory             0 (0%)      0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason              Age    From     Message
  ----    ------              ----   ----     -------
  Normal  NodeNotSchedulable  4m47s  kubelet  Node node-1 status is now: NodeNotSchedulable
root@Master:/home/labsuser#  


………………………………………………………………………………….
PRACTICE CORDON – DRAIN – TAINT
See note  date:- 17.5.2024

Plan -1
root@Master:/home/labsuser#
 kubectl create deploy mydeploy --image=nginx:latest --port=80 --replicas=10 -o yaml > deploy.yaml
root@Master:/home/labsuser# ls
DCV-Storage  Documents  Music     Public     Videos                              containerd-1.6.8-linux-amd64.tar.gz  pod.yaml   pod3.yaml  runc.amd64
Desktop      Downloads  Pictures  Templates  cni-plugins-linux-amd64-v1.1.1.tgz  deploy.yaml                          pod2.yaml  pod4.yaml  snap
root@Master:/home/labsuser# kubectl get all
NAME                            READY   STATUS    RESTARTS   AGE
pod/mydeploy-5b477bf648-4cxng   1/1     Running   0          20s
pod/mydeploy-5b477bf648-9tjk4   1/1     Running   0          20s
pod/mydeploy-5b477bf648-b8qct   1/1     Running   0          20s
pod/mydeploy-5b477bf648-cblh7   1/1     Running   0          20s
pod/mydeploy-5b477bf648-djjdc   1/1     Running   0          20s
pod/mydeploy-5b477bf648-kw89x   1/1     Running   0          20s
pod/mydeploy-5b477bf648-wppqj   1/1     Running   0          20s
pod/mydeploy-5b477bf648-xxxqr   1/1     Running   0          20s
pod/mydeploy-5b477bf648-zpnw2   1/1     Running   0          20s
pod/mydeploy-5b477bf648-zvpc9   1/1     Running   0          20s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   24h

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mydeploy   10/10   10           10          20s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/mydeploy-5b477bf648   10        10        10      20s
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-4cxng   1/1     Running   0          77s   192.168.247.13   node-2   <none>           <none>
mydeploy-5b477bf648-9tjk4   1/1     Running   0          77s   192.168.84.139   node-1   <none>           <none>
mydeploy-5b477bf648-b8qct   1/1     Running   0          77s   192.168.84.135   node-1   <none>           <none>
mydeploy-5b477bf648-cblh7   1/1     Running   0          77s   192.168.247.10   node-2   <none>           <none>
mydeploy-5b477bf648-djjdc   1/1     Running   0          77s   192.168.84.136   node-1   <none>           <none>
mydeploy-5b477bf648-kw89x   1/1     Running   0          77s   192.168.247.12   node-2   <none>           <none>
mydeploy-5b477bf648-wppqj   1/1     Running   0          77s   192.168.247.14   node-2   <none>           <none>
mydeploy-5b477bf648-xxxqr   1/1     Running   0          77s   192.168.84.137   node-1   <none>           <none>
mydeploy-5b477bf648-zpnw2   1/1     Running   0          77s   192.168.247.11   node-2   <none>           <none>
mydeploy-5b477bf648-zvpc9   1/1     Running   0          77s   192.168.84.138   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl get no -o wide
NAME     STATUS   ROLES           AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
master   Ready    control-plane   24h   v1.28.2   172.31.31.141   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
node-1   Ready    <none>          24h   v1.28.2   172.31.22.151   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
node-2   Ready    <none>          24h   v1.28.2   172.31.30.185   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
root@Master:/home/labsuser# kubectl describe no node-1
Name:               node-1
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node-1
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.31.22.151/20
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.84.128
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 16 May 2024 14:22:47 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  node-1
  AcquireTime:     <unset>
  RenewTime:       Fri, 17 May 2024 14:29:51 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 17 May 2024 14:10:55 +0000   Fri, 17 May 2024 14:10:55 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 17 May 2024 14:26:05 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 17 May 2024 14:26:05 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 17 May 2024 14:26:05 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 17 May 2024 14:26:05 +0000   Thu, 16 May 2024 14:22:47 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.31.22.151
  Hostname:    node-1
Capacity:
  cpu:                2
  ephemeral-storage:  20134592Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8019628Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  18556039957
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7917228Ki
  pods:               110
System Info:
  Machine ID:                 ec23f4405d7a5a649897db3034c844a3
  System UUID:                ec2d9506-1341-104c-cdd8-caa7ad48451e
  Boot ID:                    4d7855df-997d-4149-a929-8bfb981d0da5
  Kernel Version:             6.2.0-1013-aws
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.8
  Kubelet Version:            v1.28.2
  Kube-Proxy Version:         v1.28.2
Non-terminated Pods:          (7 in total)
  Namespace                   Name                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                         ------------  ----------  ---------------  -------------  ---
  default                     mydeploy-5b477bf648-9tjk4    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m36s
  default                     mydeploy-5b477bf648-b8qct    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m36s
  default                     mydeploy-5b477bf648-djjdc    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m36s
  default                     mydeploy-5b477bf648-xxxqr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m36s
  default                     mydeploy-5b477bf648-zvpc9    0 (0%)        0 (0%)      0 (0%)           0 (0%)         3m36s
  kube-system                 calico-node-7dh66            250m (12%)    0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-proxy-jndvz             0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                250m (12%)  0 (0%)
  memory             0 (0%)      0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 19m                kube-proxy       
  Normal   Starting                 19m                kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      19m                kubelet          invalid capacity 0 on image filesystem
  Normal   NodeAllocatableEnforced  19m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure    19m (x7 over 19m)  kubelet          Node node-1 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     19m (x7 over 19m)  kubelet          Node node-1 status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory  19m (x8 over 19m)  kubelet          Node node-1 status is now: NodeHasSufficientMemory
  Normal   RegisteredNode           18m                node-controller  Node node-1 event: Registered Node node-1 in Controller
  Normal   NodeSchedulable          6m6s               kubelet          Node node-1 status is now: NodeSchedulable
root@Master:/home/labsuser# kubectl describe no node-2
Name:               node-2
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node-2
                    kubernetes.io/os=linux
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.31.30.185/20
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.247.0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 16 May 2024 14:22:52 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  node-2
  AcquireTime:     <unset>
  RenewTime:       Fri, 17 May 2024 14:33:57 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Fri, 17 May 2024 14:10:57 +0000   Fri, 17 May 2024 14:10:57 +0000   CalicoIsUp                   Calico is running on this node
  MemoryPressure       False   Fri, 17 May 2024 14:31:11 +0000   Thu, 16 May 2024 14:22:52 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Fri, 17 May 2024 14:31:11 +0000   Thu, 16 May 2024 14:22:52 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Fri, 17 May 2024 14:31:11 +0000   Thu, 16 May 2024 14:22:52 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Fri, 17 May 2024 14:31:11 +0000   Fri, 17 May 2024 06:00:03 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.31.30.185
  Hostname:    node-2
Capacity:
  cpu:                2
  ephemeral-storage:  20134592Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8019636Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  18556039957
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7917236Ki
  pods:               110
System Info:
  Machine ID:                 ec23f4405d7a5a649897db3034c844a3
  System UUID:                ec21e678-cb3f-2e71-7ec6-70c3beba75a3
  Boot ID:                    5695ea42-89c2-4433-9504-59c5adc7c5b3
  Kernel Version:             6.2.0-1013-aws
  OS Image:                   Ubuntu 22.04.3 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.8
  Kubelet Version:            v1.28.2
  Kube-Proxy Version:         v1.28.2
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                        CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                        ------------  ----------  ---------------  -------------  ---
  default                     mydeploy-5b477bf648-4cxng                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m40s
  default                     mydeploy-5b477bf648-cblh7                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m40s
  default                     mydeploy-5b477bf648-kw89x                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m40s
  default                     mydeploy-5b477bf648-wppqj                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m40s
  default                     mydeploy-5b477bf648-zpnw2                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         7m40s
  kube-system                 calico-kube-controllers-658d97c59c-s2t2d    0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 calico-node-c4qpv                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-proxy-b48pv                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                250m (12%)  0 (0%)
  memory             0 (0%)      0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 23m                kube-proxy       
  Normal   Starting                 23m                kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      23m                kubelet          invalid capacity 0 on image filesystem
  Normal   NodeAllocatableEnforced  23m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasNoDiskPressure    23m (x7 over 23m)  kubelet          Node node-2 status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     23m (x7 over 23m)  kubelet          Node node-2 status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory  23m (x8 over 23m)  kubelet          Node node-2 status is now: NodeHasSufficientMemory
  Normal   RegisteredNode           22m                node-controller  Node node-2 event: Registered Node node-2 in Controller
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# kubectl get no
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   24h   v1.28.2
node-1   Ready    <none>          24h   v1.28.2
node-2   Ready    <none>          24h   v1.28.2
root@Master:/home/labsuser# kubectl drain node-2
node/node-2 cordoned
error: unable to drain node "node-2" due to error:cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-c4qpv, kube-system/kube-proxy-b48pv, continuing command...
There are pending nodes to be drained:
 node-2
cannot delete DaemonSet-managed Pods (use --ignore-daemonsets to ignore): kube-system/calico-node-c4qpv, kube-system/kube-proxy-b48pv
root@Master:/home/labsuser# kubectl drain node-2 --ignore-daemonsets
node/node-2 already cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/calico-node-c4qpv, kube-system/kube-proxy-b48pv
evicting pod kube-system/calico-kube-controllers-658d97c59c-s2t2d
evicting pod default/mydeploy-5b477bf648-kw89x
evicting pod default/mydeploy-5b477bf648-wppqj
evicting pod default/mydeploy-5b477bf648-zpnw2
evicting pod default/mydeploy-5b477bf648-4cxng
evicting pod default/mydeploy-5b477bf648-cblh7
pod/mydeploy-5b477bf648-4cxng evicted
pod/mydeploy-5b477bf648-zpnw2 evicted
pod/mydeploy-5b477bf648-kw89x evicted
pod/mydeploy-5b477bf648-cblh7 evicted
pod/mydeploy-5b477bf648-wppqj evicted
pod/calico-kube-controllers-658d97c59c-s2t2d evicted
node/node-2 drained
root@Master:/home/labsuser# kubectl get po -o wide -n kube-system
NAME                                       READY   STATUS    RESTARTS      AGE     IP               NODE     NOMINATED NODE   READINESS GATES
calico-kube-controllers-658d97c59c-tlpdn   1/1     Running   0             2m45s   192.168.84.144   node-1   <none>           <none>
calico-node-7dh66                          1/1     Running   2 (30m ago)   24h     172.31.22.151    node-1   <none>           <none>
calico-node-c4qpv                          1/1     Running   2 (30m ago)   24h     172.31.30.185    node-2   <none>           <none>
calico-node-lkqnt                          1/1     Running   2 (30m ago)   24h     172.31.31.141    master   <none>           <none>
coredns-5dd5756b68-kdvzr                   1/1     Running   2 (30m ago)   24h     192.168.219.69   master   <none>           <none>
coredns-5dd5756b68-w5sct                   1/1     Running   2 (30m ago)   24h     192.168.219.70   master   <none>           <none>
etcd-master                                1/1     Running   2 (30m ago)   24h     172.31.31.141    master   <none>           <none>
kube-apiserver-master                      1/1     Running   2 (30m ago)   24h     172.31.31.141    master   <none>           <none>
kube-controller-manager-master             1/1     Running   2 (30m ago)   24h     172.31.31.141    master   <none>           <none>
kube-proxy-b48pv                           1/1     Running   2 (30m ago)   24h     172.31.30.185    node-2   <none>           <none>
kube-proxy-d4djj                           1/1     Running   2 (30m ago)   24h     172.31.31.141    master   <none>           <none>
kube-proxy-jndvz                           1/1     Running   2 (30m ago)   24h     172.31.22.151    node-1   <none>           <none>
kube-scheduler-master                      1/1     Running   2 (30m ago)   24h     172.31.31.141    master   <none>           <none>
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# kubectl get all
NAME                            READY   STATUS    RESTARTS   AGE
pod/mydeploy-5b477bf648-9tjk4   1/1     Running   0          16m
pod/mydeploy-5b477bf648-b8qct   1/1     Running   0          16m
pod/mydeploy-5b477bf648-bwvxq   1/1     Running   0          4m4s
pod/mydeploy-5b477bf648-djjdc   1/1     Running   0          16m
pod/mydeploy-5b477bf648-fvbtj   1/1     Running   0          4m4s
pod/mydeploy-5b477bf648-gdcj7   1/1     Running   0          4m4s
pod/mydeploy-5b477bf648-p2rpp   1/1     Running   0          4m4s
pod/mydeploy-5b477bf648-r9l6k   1/1     Running   0          4m4s
pod/mydeploy-5b477bf648-xxxqr   1/1     Running   0          16m
pod/mydeploy-5b477bf648-zvpc9   1/1     Running   0          16m

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   24h

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mydeploy   10/10   10           10          16m

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/mydeploy-5b477bf648   10        10        10      16m
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-9tjk4   1/1     Running   0          20m     192.168.84.139   node-1   <none>           <none>
mydeploy-5b477bf648-b8qct   1/1     Running   0          20m     192.168.84.135   node-1   <none>           <none>
mydeploy-5b477bf648-bwvxq   1/1     Running   0          8m23s   192.168.84.141   node-1   <none>           <none>
mydeploy-5b477bf648-djjdc   1/1     Running   0          20m     192.168.84.136   node-1   <none>           <none>
mydeploy-5b477bf648-fvbtj   1/1     Running   0          8m23s   192.168.84.145   node-1   <none>           <none>
mydeploy-5b477bf648-gdcj7   1/1     Running   0          8m23s   192.168.84.140   node-1   <none>           <none>
mydeploy-5b477bf648-p2rpp   1/1     Running   0          8m23s   192.168.84.143   node-1   <none>           <none>
mydeploy-5b477bf648-r9l6k   1/1     Running   0          8m23s   192.168.84.142   node-1   <none>           <none>
mydeploy-5b477bf648-xxxqr   1/1     Running   0          20m     192.168.84.137   node-1   <none>           <none>
mydeploy-5b477bf648-zvpc9   1/1     Running   0          20m     192.168.84.138   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl get nodes
NAME     STATUS                     ROLES           AGE   VERSION
master   Ready                      control-plane   24h   v1.28.2
node-1   Ready                      <none>          24h   v1.28.2
node-2   Ready,SchedulingDisabled   <none>          24h   v1.28.2
root@Master:/home/labsuser# TO UNDRAIN NODE-2
TO: command not found
root@Master:/home/labsuser# # TO UNDRAIN NODE-2
root@Master:/home/labsuser# kubectl uncordon node-2
node/node-2 uncordoned
root@Master:/home/labsuser# kubectl get nodes
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   24h   v1.28.2
node-1   Ready    <none>          24h   v1.28.2
node-2   Ready    <none>          24h   v1.28.2
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE   IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-9tjk4   1/1     Running   0          33m   192.168.84.139   node-1   <none>           <none>
mydeploy-5b477bf648-b8qct   1/1     Running   0          33m   192.168.84.135   node-1   <none>           <none>
mydeploy-5b477bf648-bwvxq   1/1     Running   0          21m   192.168.84.141   node-1   <none>           <none>
mydeploy-5b477bf648-djjdc   1/1     Running   0          33m   192.168.84.136   node-1   <none>           <none>
mydeploy-5b477bf648-fvbtj   1/1     Running   0          21m   192.168.84.145   node-1   <none>           <none>
mydeploy-5b477bf648-gdcj7   1/1     Running   0          21m   192.168.84.140   node-1   <none>           <none>
mydeploy-5b477bf648-p2rpp   1/1     Running   0          21m   192.168.84.143   node-1   <none>           <none>
mydeploy-5b477bf648-r9l6k   1/1     Running   0          21m   192.168.84.142   node-1   <none>           <none>
mydeploy-5b477bf648-xxxqr   1/1     Running   0          33m   192.168.84.137   node-1   <none>           <none>
mydeploy-5b477bf648-zvpc9   1/1     Running   0          33m   192.168.84.138   node-1   <none>           <none>
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# kubectl get no node-2
NAME     STATUS   ROLES    AGE   VERSION
node-2   Ready    <none>   24h   v1.28.2
root@Master:/home/labsuser# kubectl get no node-2 -o wide
NAME     STATUS   ROLES    AGE   VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
node-2   Ready    <none>   24h   v1.28.2   172.31.30.185   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
root@Master:/home/labsuser#

…………………………………………………………………………………………………………………………………………….

Plan -2
root@Master:/home/labsuser# kubectl create deploy mydeploy --image=nginx:latest --port=80 --replicas=10
deployment.apps/mydeploy created
root@Master:/home/labsuser# kubectl get all
NAME                            READY   STATUS    RESTARTS   AGE
pod/mydeploy-5b477bf648-2jgcn   1/1     Running   0          7s
pod/mydeploy-5b477bf648-7p7b9   1/1     Running   0          7s
pod/mydeploy-5b477bf648-84l5h   1/1     Running   0          7s
pod/mydeploy-5b477bf648-bcm59   1/1     Running   0          7s
pod/mydeploy-5b477bf648-nwhf4   1/1     Running   0          7s
pod/mydeploy-5b477bf648-qrhmq   1/1     Running   0          7s
pod/mydeploy-5b477bf648-sfpcz   1/1     Running   0          7s
pod/mydeploy-5b477bf648-ssh2x   1/1     Running   0          7s
pod/mydeploy-5b477bf648-vjhrd   1/1     Running   0          7s
pod/mydeploy-5b477bf648-zm4pf   1/1     Running   0          7s

NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   24h

NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/mydeploy   10/10   10           10          7s

NAME                                  DESIRED   CURRENT   READY   AGE
replicaset.apps/mydeploy-5b477bf648   10        10        10      7s
root@Master:/home/labsuser# kubectl get no
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   24h   v1.28.2
node-1   Ready    <none>          24h   v1.28.2
node-2   Ready    <none>          24h   v1.28.2
root@Master:/home/labsuser# kubectl taint node node-1 K=V:NoSchedule
node/node-1 tainted
root@Master:/home/labsuser# kubectl get nodes -o custom-columns=NAME:metadata.name,TAINT:spec.taints[].effect,KEY:spec.taints[].key,VALUE:spec.taints[*].value
NAME     TAINT        KEY                                     VALUE
master   NoSchedule   node-role.kubernetes.io/control-plane   <none>
node-1   NoSchedule   K                                       V
node-2   <none>       <none>                                  <none>
root@Master:/home/labsuser# 
root@Master:/home/labsuser# 
root@Master:/home/labsuser# kubectl drain node-2 --ignore-daemonsets
node/node-2 cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/calico-node-c4qpv, kube-system/kube-proxy-b48pv
evicting pod default/mydeploy-5b477bf648-7p7b9
evicting pod default/mydeploy-5b477bf648-zm4pf
evicting pod default/mydeploy-5b477bf648-bcm59
evicting pod default/mydeploy-5b477bf648-84l5h
evicting pod default/mydeploy-5b477bf648-vjhrd
pod/mydeploy-5b477bf648-bcm59 evicted
pod/mydeploy-5b477bf648-7p7b9 evicted
pod/mydeploy-5b477bf648-84l5h evicted
pod/mydeploy-5b477bf648-vjhrd evicted
pod/mydeploy-5b477bf648-zm4pf evicted
node/node-2 drained
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE    IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-2jgcn   1/1     Running   0          4m6s   192.168.84.150   node-1   <none>           <none>
mydeploy-5b477bf648-2qx9h   0/1     Pending   0          24s    <none>           <none>   <none>           <none>
mydeploy-5b477bf648-64lhr   0/1     Pending   0          24s    <none>           <none>   <none>           <none>
mydeploy-5b477bf648-794f7   0/1     Pending   0          25s    <none>           <none>   <none>           <none>
mydeploy-5b477bf648-fz9k6   0/1     Pending   0          24s    <none>           <none>   <none>           <none>
mydeploy-5b477bf648-hjqd7   0/1     Pending   0          24s    <none>           <none>   <none>           <none>
mydeploy-5b477bf648-nwhf4   1/1     Running   0          4m6s   192.168.84.148   node-1   <none>           <none>
mydeploy-5b477bf648-qrhmq   1/1     Running   0          4m6s   192.168.84.149   node-1   <none>           <none>
mydeploy-5b477bf648-sfpcz   1/1     Running   0          4m6s   192.168.84.146   node-1   <none>           <none>
mydeploy-5b477bf648-ssh2x   1/1     Running   0          4m6s   192.168.84.147   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl uncordon node-2
node/node-2 uncordoned
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS              RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-2jgcn   1/1     Running             0          6m43s   192.168.84.150   node-1   <none>           <none>
mydeploy-5b477bf648-2qx9h   0/1     ContainerCreating   0          3m1s    <none>           node-2   <none>           <none>
mydeploy-5b477bf648-64lhr   1/1     Running             0          3m1s    192.168.247.21   node-2   <none>           <none>
mydeploy-5b477bf648-794f7   1/1     Running             0          3m2s    192.168.247.22   node-2   <none>           <none>
mydeploy-5b477bf648-fz9k6   1/1     Running             0          3m1s    192.168.247.20   node-2   <none>           <none>
mydeploy-5b477bf648-hjqd7   1/1     Running             0          3m1s    192.168.247.23   node-2   <none>           <none>
mydeploy-5b477bf648-nwhf4   1/1     Running             0          6m43s   192.168.84.148   node-1   <none>           <none>
mydeploy-5b477bf648-qrhmq   1/1     Running             0          6m43s   192.168.84.149   node-1   <none>           <none>
mydeploy-5b477bf648-sfpcz   1/1     Running             0          6m43s   192.168.84.146   node-1   <none>           <none>
mydeploy-5b477bf648-ssh2x   1/1     Running             0          6m43s   192.168.84.147   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl drain node-2 --ignore-daemonsets
node/node-2 cordoned
Warning: ignoring DaemonSet-managed Pods: kube-system/calico-node-c4qpv, kube-system/kube-proxy-b48pv
evicting pod default/mydeploy-5b477bf648-hjqd7
evicting pod default/mydeploy-5b477bf648-2qx9h
evicting pod default/mydeploy-5b477bf648-64lhr
evicting pod default/mydeploy-5b477bf648-794f7
evicting pod default/mydeploy-5b477bf648-fz9k6
pod/mydeploy-5b477bf648-hjqd7 evicted
pod/mydeploy-5b477bf648-64lhr evicted
pod/mydeploy-5b477bf648-794f7 evicted
pod/mydeploy-5b477bf648-2qx9h evicted
pod/mydeploy-5b477bf648-fz9k6 evicted
node/node-2 drained
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-2jgcn   1/1     Running   0          8m12s   192.168.84.150   node-1   <none>           <none>
mydeploy-5b477bf648-7tr8k   0/1     Pending   0          7s      <none>           <none>   <none>           <none>
mydeploy-5b477bf648-lcxtj   0/1     Pending   0          7s      <none>           <none>   <none>           <none>
mydeploy-5b477bf648-nwhf4   1/1     Running   0          8m12s   192.168.84.148   node-1   <none>           <none>
mydeploy-5b477bf648-qrhmq   1/1     Running   0          8m12s   192.168.84.149   node-1   <none>           <none>
mydeploy-5b477bf648-sfpcz   1/1     Running   0          8m12s   192.168.84.146   node-1   <none>           <none>
mydeploy-5b477bf648-ssh2x   1/1     Running   0          8m12s   192.168.84.147   node-1   <none>           <none>
mydeploy-5b477bf648-vtjhz   0/1     Pending   0          7s      <none>           <none>   <none>           <none>
mydeploy-5b477bf648-xk4wk   0/1     Pending   0          7s      <none>           <none>   <none>           <none>
mydeploy-5b477bf648-zsct7   0/1     Pending   0          7s      <none>           <none>   <none>           <none>
root@Master:/home/labsuser# kubectl taint node node-1 k-
error: taint "k" not found
root@Master:/home/labsuser# kubectl taint node node-1 K-
node/node-1 untainted
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-2jgcn   1/1     Running   0          14m     192.168.84.150   node-1   <none>           <none>
mydeploy-5b477bf648-7tr8k   1/1     Running   0          6m30s   192.168.84.154   node-1   <none>           <none>
mydeploy-5b477bf648-lcxtj   1/1     Running   0          6m30s   192.168.84.152   node-1   <none>           <none>
mydeploy-5b477bf648-nwhf4   1/1     Running   0          14m     192.168.84.148   node-1   <none>           <none>
mydeploy-5b477bf648-qrhmq   1/1     Running   0          14m     192.168.84.149   node-1   <none>           <none>
mydeploy-5b477bf648-sfpcz   1/1     Running   0          14m     192.168.84.146   node-1   <none>           <none>
mydeploy-5b477bf648-ssh2x   1/1     Running   0          14m     192.168.84.147   node-1   <none>           <none>
mydeploy-5b477bf648-vtjhz   1/1     Running   0          6m30s   192.168.84.155   node-1   <none>           <none>
mydeploy-5b477bf648-xk4wk   1/1     Running   0          6m30s   192.168.84.151   node-1   <none>           <none>
mydeploy-5b477bf648-zsct7   1/1     Running   0          6m30s   192.168.84.153   node-1   <none>           <none>
root@Master:/home/labsuser# kubectl get no
NAME     STATUS                     ROLES           AGE   VERSION
master   Ready                      control-plane   25h   v1.28.2
node-1   Ready                      <none>          25h   v1.28.2
node-2   Ready,SchedulingDisabled   <none>          25h   v1.28.2
root@Master:/home/labsuser# kubectl uncordon node-2
node/node-2 uncordoned
root@Master:/home/labsuser# kubectl get no
NAME     STATUS   ROLES           AGE   VERSION
master   Ready    control-plane   25h   v1.28.2
node-1   Ready    <none>          25h   v1.28.2
node-2   Ready    <none>          25h   v1.28.2
root@Master:/home/labsuser# kubectl get po -o wide
NAME                        READY   STATUS    RESTARTS   AGE     IP               NODE     NOMINATED NODE   READINESS GATES
mydeploy-5b477bf648-2jgcn   1/1     Running   0          15m     192.168.84.150   node-1   <none>           <none>
mydeploy-5b477bf648-7tr8k   1/1     Running   0          7m52s   192.168.84.154   node-1   <none>           <none>
mydeploy-5b477bf648-lcxtj   1/1     Running   0          7m52s   192.168.84.152   node-1   <none>           <none>
mydeploy-5b477bf648-nwhf4   1/1     Running   0          15m     192.168.84.148   node-1   <none>           <none>
mydeploy-5b477bf648-qrhmq   1/1     Running   0          15m     192.168.84.149   node-1   <none>           <none>
mydeploy-5b477bf648-sfpcz   1/1     Running   0          15m     192.168.84.146   node-1   <none>           <none>
mydeploy-5b477bf648-ssh2x   1/1     Running   0          15m     192.168.84.147   node-1   <none>           <none>
mydeploy-5b477bf648-vtjhz   1/1     Running   0          7m52s   192.168.84.155   node-1   <none>           <none>
mydeploy-5b477bf648-xk4wk   1/1     Running   0          7m52s   192.168.84.151   node-1   <none>           <none>
mydeploy-5b477bf648-zsct7   1/1     Running   0          7m52s   192.168.84.153   node-1   <none>           <none>
root@Master:/home/labsuser#

………………………………………………………………………..
Plan – 3


…………………………………….
DATE: 18.5.2023
 k get po -n kube-system -o wide

 k get ds -n kube-system
vi ds.yaml
  255  kubectl apply -f ds.yaml 
  256  vi ds.yaml
  257  kubectl apply -f ds.yaml 
  258  kubectl get all
  259  kubectl get po
  260  kubectl get po -o wide
  261  kubectl describe no
  262  vi ds.yaml
  263  kubectl get all
  264  kubectl delete ds fluentd-daemonset-1
  265  kubectl get all
  266  kubectl get po -o wide
  267  kubectl get 
  268  kubectl get all
  269  kubectl apply -f ds.yaml 
  270  kubectl get all
  271  kubectl get po -o wide
  272  kubectl get ds -n kube-system
  273  kubectl get ds -n kube-system -o wide
  274  kubectl taint no master node-role.kubernetes.io/control-plane:NoSchedule-
  275  kubectl delete ds fluentd-daemonset-1
  276  kubectl get ds
  277  kubectl get po -o wide
  278  kubectl apply -f ds.yaml 
  279  kubectl get po -o wide
  280  kubectl taint no master node-role.kubernetes.io/control-plane:NoSchedule
  281  kubectl apply -f ds.yaml 
  282  kubectl get po -o wide
  283  kubectl get ds
  284  kubectl get po -o wide
  285  kubectl delete po fluentd-daemonset-1-779mn
  286  kubectl get po -o wide
  287  kubectl delete po fluentd-daemonset-1-qqqs6 fluentd-daemonset-1-bppqz fluentd-daemonset-1-p6dpq
  288  kubectl get po -o wide
  289  history
root@Master:/home/labsuser#


DATE: 19.5.2024
How to get a Docker container's IP address from the host
Stack overflow

Use the container ID to run:
docker inspect <container ID>
At the bottom, under NetworkSettings, you can find IPAddress
Or just do for UNIX based:
docker inspect <container id> | grep "IPAddress"
And for Windows CMD:
docker inspect <container id> | findstr "IPAddress"


DATE: 23.5.2024    SERVICE PRACTICE

kubectl get pods --output=wide

DATE: 23.5.2024    SERVICE PRACTICE

root@Master:/home/labsuser/serviceDemo/deploy# k get all
NAME        READY   STATUS    RESTARTS      AGE
pod/mysql   1/1     Running   0             34m
pod/web1    2/2     Running   2 (34m ago)   34m

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP        5d17h
service/mysql        ClusterIP   10.107.77.244   <none>        3306/TCP       34m
service/web          NodePort    10.96.54.54     <none>        80:32197/TCP   34m
root@Master:/home/labsuser/serviceDemo/deploy# k describe service web
Name:                     web
Namespace:                default
Labels:                   app=demo
                          name=web
Annotations:              <none>
Selector:                 name=web
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.54.54
IPs:                      10.96.54.54
Port:                     http  80/TCP
TargetPort:               5000/TCP
NodePort:                 http  32197/TCP
Endpoints:                192.168.104.25:5000
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
root@Master:/home/labsuser/serviceDemo/deploy# k get no - o wide
Error from server (NotFound): nodes "-" not found
Error from server (NotFound): nodes "o" not found
Error from server (NotFound): nodes "wide" not found
root@Master:/home/labsuser/serviceDemo/deploy# k get no -o wide
NAME     STATUS   ROLES           AGE     VERSION   INTERNAL-IP     EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION   CONTAINER-RUNTIME
master   Ready    control-plane   5d17h   v1.28.2   172.31.47.194   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
node1    Ready    <none>          5d17h   v1.28.2   172.31.47.235   <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
node2    Ready    <none>          5d17h   v1.28.2   172.31.35.32    <none>        Ubuntu 22.04.3 LTS   6.2.0-1013-aws   containerd://1.6.8
root@Master:/home/labsuser/serviceDemo/deploy# k get po -o wide
NAME    READY   STATUS    RESTARTS      AGE   IP               NODE    NOMINATED NODE   READINESS GATES
mysql   1/1         Running   0             36m   192.168.104.24   node2   <none>           <none>
web1    2/2         Running   2 (36m ago)   36m   192.168.104.25   node2   <none>           <none>
root@Master:/home/labsuser/serviceDemo/deploy#


services
history | cut -c 8-

git clone https://github.com/rskTech/serviceDemo.git
k apply -f db-pod.yml 
k apply -f db-svc.yml 
k apply -f web-pod.yaml
k apply -f web-svc.yml 
Initialize the database with sample schema
curl  172.31.35.32:32404/init
Insert some sample data

curl http://172.31.47.235:32404/users/4
curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "1", "user":"JOHN"}' http://172.31.47.235:32404/users/add
curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "2", "user":"KATRINA"}' http://172.31.47.235:32404/users/add
curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "3", "user":"CRUZ"}' http://172.31.47.235:32404/users/add
curl -i -H "Content-Type: application/json" -X POST -d '{"uid": "4", "user":"Mike Taylor"}' http://172.31.47.235:32404/users/add
curl http://172.31.47.235:32404/users/1
curl http://172.31.47.235:32404/users/1
curl http://172.31.47.235:32404/users/2
curl http://172.31.47.235:32404/users/2
curl http://172.31.47.235:32404/users/3
curl http://172.31.47.235:32404/users/3
curl http://172.31.47.235:32404/users/4
curl http://172.31.47.235:32404/users/4

root@Master:/home/labsuser# k get all
NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE
service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   6d2h
root@Master:/home/labsuser# k describe service kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.0.1
IPs:               10.96.0.1
Port:              https  443/TCP
TargetPort:        6443/TCP
Endpoints:         172.31.47.194:6443
Session Affinity:  None
Events:            <none>
root@Master:/home/labsuser#


DATE - 1/6/2024

# DATE - 1/6/2024
k expose po nginx --name mysvc --type=NodePort --port=80 --target-port=80 --dry-run=client -o yaml > service.yal
alias k=kubectl
k expose po nginx --name mysvc --type=NodePort --port=80 --target-port=80 --dry-run=client -o yaml > service.yaml
ls
vi service.yaml 
cat  service.yaml

apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: mysvc
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: nginx
  type: NodePort
status:
  loadBalancer: {}

https://github.com/rskTech/k8s_material/blob/master/practiceset

root@Master:/home/labsuser#   k get all -A 
NAMESPACE     NAME                                           READY   STATUS    RESTARTS        AGE
default       pod/nginx                                      1/1     Running   1 (32m ago)     6h29m
kube-system   pod/calico-kube-controllers-658d97c59c-w898g   1/1     Running   9 (4h55m ago)   9d
kube-system   pod/calico-node-22wm2                          1/1     Running   9 (32m ago)     9d
kube-system   pod/calico-node-mln57                          1/1     Running   9 (4h55m ago)   9d
kube-system   pod/calico-node-pvl9k                          1/1     Running   9 (32m ago)     9d
kube-system   pod/coredns-5dd5756b68-4dc7x                   1/1     Running   9 (32m ago)     9d
kube-system   pod/coredns-5dd5756b68-h2qt4                   1/1     Running   9 (32m ago)     9d
kube-system   pod/etcd-master                                1/1     Running   9 (32m ago)     9d
kube-system   pod/kube-apiserver-master                      1/1     Running   9 (32m ago)     9d
kube-system   pod/kube-controller-manager-master             1/1     Running   9 (32m ago)     9d
kube-system   pod/kube-proxy-bnh87                           1/1     Running   9 (4h55m ago)   9d
kube-system   pod/kube-proxy-k2qs5                           1/1     Running   9 (32m ago)     9d
kube-system   pod/kube-proxy-qhwj8                           1/1     Running   9 (32m ago)     9d
kube-system   pod/kube-scheduler-master                      1/1     Running   9 (32m ago)     9d

NAMESPACE     NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                  AGE
default       service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP                  9d
default       service/mysvc        NodePort    10.100.235.87   <none>        80:32125/TCP             6h25m
kube-system   service/kube-dns     ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP   9d

NAMESPACE     NAME                         DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/calico-node   3         3         3       3            3           kubernetes.io/os=linux   9d
kube-system   daemonset.apps/kube-proxy    3         3         3       3            3           kubernetes.io/os=linux   9d

NAMESPACE     NAME                                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/calico-kube-controllers   1/1     1            1           9d
kube-system   deployment.apps/coredns                   2/2     2            2           9d

NAMESPACE     NAME                                                 DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/calico-kube-controllers-658d97c59c   1         1         1       9d
kube-system   replicaset.apps/coredns-5dd5756b68                   2         2         2       9d
root@Master:/home/labsuser#

# practiceset
# BASIC CORE CONCEPT Questions
# List all the namespaces in the cluster
kubectl get ns
# List all the pods in all namespaces
kubectl get ns -o wide
k get ns -A
k get po -A
k -h
k get po -h
k get po
k get po -o wide
k get po -o wide -ns
k get po -o wide -n default
k get po -A
k get po all
k get all
k get all -A
k get po -h
k get ns
alias k=kubectl
k get ns
k -help
k --help
kget po -h
k get po -h
clear
k -h
clear
k get po -h
k get po -A
k get po -A -o wide
k get all -h
k get all -A 
k get all -A -o wide
kubectl get ns
k get po -n default
k get po -n kube-node-lease
k get po -n kube-public
k get po -n kube-system
k get svc
k get svc -o wide
k get svc -A
k get svc -h
k get svc -A
k get svc -A -o wide
k get svc
k get all
k -h
k get json -h
# ###########################################################
k get json -h
kubectl get pods -o json
kubectl get pods -o=jsonpath='{@}'
kubectl get pods -o=jsonpath='{.items[0]}'
kubectl get pods -o=jsonpath='{.items[0].metadata.name}' or  
kubectl get pods -o=jsonpath='{.items[*].metadata.name}'
ls
kubectl get pods -o=jsonpath="{.items[*]['metadata.name', 'status.capacity']}"
kubectl get pods -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.startTime}{"\n"}{end}'
kubectl get pods -o=jsonpath='{.items[0].metadata.labels.kubernetes\.io/hostname}'

# TO GET POD NAME
kubectl get pods -o=jsonpath="{.items[*]['metadata.name']}"
# TO GET POD NAME AND NAME SPACE
kubectl get pods -o=jsonpath="{.items[*]['metadata.name', 'metadata.namespace']}"  # 

# CKA: get pod name, namespace, and container image using jsonpath + kodekloud     #  rob_kodekloud
root@Master:/home/labsuser#       
k get po -o custom-columns='N:.metadata.name,NS:.metadata.namespace,I:.spec.containers[].image'

ALSO READ THE FOLLOWING:

https://github.com/kodekloudhub/community-faq/blob/main/docs/jsonpath.md




Compare the two pod.yaml
pod.yaml

kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: test-pod
spec:
  containers:
  - image: nginx
    name: nginx-test-image
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


POD-2.yaml

apiVersion: v1
kind: Pod
metadata:
    labels:
      run: nginx
    name: test-pod-2
spec:
  containers:
  - image: nginx
    name: nginx-test-image-2
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}



DATE : - 04/6/2024
Q ) How to copy a file from current directory to a Kubernetes container running under a deployment ?
Source – Gemini AI
You can copy a file from your current directory to a Kubernetes container running under a deployment using the kubectl cp command. Here's how:
Requirements:
•	kubectl: You need kubectl installed and configured to connect to your Kubernetes cluster.
•	tar: The tar binary needs to be present in the container image you're copying the file to.
Steps:
1.	Identify the Deployment and Container:
o	Use kubectl get deployments to find the name of the deployment you want to access.
o	Optionally, use kubectl get pods to identify the specific pod name within the deployment (though kubectl cp can work with deployments too).
2.	Copy the File:
The basic syntax for kubectl cp is:
kubectl cp <source> <destination>
o	<source>: Path to the file on your local machine (current directory in your case).
o	<destination>: Path to the destination within the container, including the target pod and container name (optional).
Here are some options depending on your situation:
o	Copy to a specific container in a pod:
o	kubectl cp <local_file> <namespace>/<pod_name>:<container_name>:/<destination_path>
	Replace <local_file> with the path to your file in your current directory.
	Replace <namespace>, <pod_name>, and <container_name> with the actual names.
	Replace <destination_path> with the desired location within the container.
o	Copy to a pod managed by a deployment (without specifying container):
o	kubectl cp <local_file> <namespace>/<deployment_name>:/<destination_path>
	This will copy the file to one of the pods managed by the deployment. Kubernetes will choose the target pod.
	Replace the placeholders with the corresponding names.
Example:
Assuming you have a deployment named my-nginx in the default namespace, and you want to copy a file named config.txt from your current directory to the /etc directory within a container named nginx in one of the pods managed by the deployment, you would run:
kubectl cp config.txt default/my-nginx:nginx:/etc/config.txt
Important Note:
•	kubectl cp requires the tar binary to be present in the container image. If it's not available, the copy operation will fail.
•	For more advanced use cases or complex directory structures, consider using kubectl exec to directly interact with the container's shell.

Date: - 6.6.2024
JOBS
FIELDS:
  activeDeadlineSeconds <integer>
    Specifies the duration in seconds relative to the startTime that the job may
    be continuously active before the system tries to terminate it; value must
    be positive integer. If a Job is suspended (at creation or through an
    update), this timer will effectively be stopped and reset when the Job is
    resumed again.
backoffLimit 
parallelism
completionMode: Indexed



Creating an Indexed Job
https://kubernetes.io/blog/2021/04/19/introducing-indexed-jobs/#creating-an-indexed-job
Read the above article before practicing.
Example- yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: 'sample-job'
spec:
  completions: 3
  parallelism: 3
  completionMode: Indexed
  template:
    spec:
      restartPolicy: Never
      containers:
      - command:
        - 'bash'
        - '-c'
        - 'echo "My partition: ${JOB_COMPLETION_INDEX}"'
        image: 'docker.io/library/bash'
        name: 'sample-load'
root@Master:/home/labsuser# cat job.yaml 
apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: myjob
spec:
  completions: 10
  parallelism: 3
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - command:
        - /bin/sh
        - -c
        - sleep 10
        image: ubuntu:20.04
        name: myjob
        resources: {}
      restartPolicy: Never
status: {}
root@Master:/home/labsuser# history | cut -c 8-
k create job myjob --image=ubuntu:20.04 --dry-run=client -o yaml -- /bin/sh -c "sleep 10" > job.yaml
kubectl explain job.spec
vi job.yaml 
k apply -f job.yaml 
watch kubectl get all
vi job-2.yaml
k apply -f job-2.yaml 
watch kubectl get all

DATE :  - 7/6/2024
Cron job :
k create cj mycj --image=ubuntu:20.04 --schedule="*/1 * * * *" --dry-run=client -o yaml -- /bin/sh -c "sleep 10" > cronjob.yaml
vi cronjob.yaml
k apply -f cronjob.yaml 
watch kubectl get all
k get cj mycj
apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: null
  name: mycj
spec:
  jobTemplate:
    metadata:
      creationTimestamp: null
      name: mycj
          template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - command:
            - /bin/sh
            - -c
            - sleep 10
            image: ubuntu:20.04
            name: mycj
            resources: {}
          restartPolicy: OnFailure
  schedule: '*/1 * * * *'
status: {}

Cron Job Yaml file with the following three job spec completions: ,parallelism and completionMode: Indexed

apiVersion: batch/v1
kind: CronJob
metadata:
  creationTimestamp: null
  name: mycj
spec:
  jobTemplate:
    metadata:
      creationTimestamp: null
      name: mycj
    spec:
      completions: 3
      parallelism: 3
      completionMode: Indexed
      template:
        metadata:
          creationTimestamp: null
        spec:
          containers:
          - command:
            - /bin/sh
            - -c
            - sleep 10
            image: ubuntu:20.04
            name: mycj
            resources: {}
          restartPolicy: OnFailure
  schedule: '*/1 * * * *'
status: {}

DATE : - 9/6/2024


k get jobs --watch
k get jobs --watch -o wide
watch kubectl get all
k get cj --watch 
k get po –watch

kubectl get job --watch
kubectl get cj --watch
kubectl get po --watch
^Croot@Master:/home/labsuser# kubectl get all --watch
error: you may only specify a single resource type

CONFIGMAPS : 


k create cm mycm --from-literal=db_host=192.168.0.2 --from-literal=db_port=8080
vi pod.yaml
k apply -f pod.yaml
k get po
 k exec -it nginx bash # the cmd will take you inside the container where you can type the cmd “ env ” and get the output.
 k exec -it nginx – env # Displays the same output on the terminal
cat pod.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    ports:
    - containerPort: 80
    env:
      - name: DBPORT
        valueFrom:
          configMapKeyRef:
            name: mycm
            key: db_port
      - name: DBHOST
        valueFrom:
          configMapKeyRef:
            name: mycm
            key: db_host
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
history | cut -c 8-
Creating two different configfiles and  passing them as environmental variables to the pod.yaml

k create cm mycm1 --from-literal=db_port=9090
k create cm mycm2 --from-literal=db_host=192.168.1.5
k get cm
k get all 
vi pod.yaml
…………………………………………………………………………………
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: nginx
  name: nginx
spec:
  containers:
  - image: nginx
    name: nginx
    ports:
    - containerPort: 80
    env:
      - name: DBPORT
        valueFrom:
          configMapKeyRef:
            name: mycm
            key: db_port
      - name: DBHOST
        valueFrom:
          configMapKeyRef:
            name: mycm
            key: db_host
    resources: {}
    volumeMounts:
      - name: myvol
        mountPath: /etc/lala
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
    - name: myvol
      configMap:
        name: mycm1
status: {}

……………………………………………………………………
k apply -f pod.yaml
k exec -it nginx bash
env
exit
k exec -it nginx -- env







